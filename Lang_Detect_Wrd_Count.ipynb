{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPanMUJnx6Avs+4Un0l51UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberthouston14/Feature_Extractors/blob/main/Lang_Detect_Wrd_Count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU configuration\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "59y22zmgSJkR",
        "outputId": "e0e0eb08-278f-4f83-e851-0f775ca6f72f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVKtx7cbR6IZ",
        "outputId": "329ed753-30cb-4726-be3d-038d4f8b777d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file location: /content/drive/MyDrive/Production Datasets/GOLD_DeDup.csv\n",
            "Enter the column name for labels: label\n",
            "Enter the column name for text: text\n",
            "Row 17371 contains meaningful text.\n",
            "Row 4685 contains meaningful text.\n",
            "Row 42268 contains meaningful text.\n",
            "Row 9714 contains meaningful text.\n",
            "Row 42626 contains meaningful text.\n",
            "Row 7910 contains meaningful text.\n",
            "Row 25942 contains meaningful text.\n",
            "Row 29124 contains meaningful text.\n",
            "Row 12057 contains meaningful text.\n",
            "Row 33186 contains meaningful text.\n",
            "Row 32932 contains meaningful text.\n",
            "Row 9902 contains meaningful text.\n",
            "Row 32917 contains meaningful text.\n",
            "Row 8986 contains meaningful text.\n",
            "Row 20279 contains meaningful text.\n",
            "Row 17960 contains meaningful text.\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# import spacy\n",
        "# import random\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# # Prompt user for the file location\n",
        "# file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# # Load the .csv file as a DataFrame\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# # Prompt user for the column names\n",
        "# label_column = input(\"Enter the column name for labels: \")\n",
        "# text_column = input(\"Enter the column name for text: \")\n",
        "\n",
        "# def has_meaningful_text(email_text):\n",
        "#     doc = nlp(email_text)\n",
        "#     noun_count = sum([1 for token in doc if token.pos_ == \"NOUN\"])\n",
        "#     return noun_count >= 10  # arbitrary threshold for number of nouns\n",
        "\n",
        "# # Downsample the dataset to select 25 rows at random\n",
        "# n_samples = 25\n",
        "# label_counts = df[label_column].value_counts()\n",
        "# n_samples_per_label = int(n_samples / 2)\n",
        "# label_1_rows = df[df[label_column] == 1].sample(n_samples_per_label)\n",
        "# label_0_rows = df[df[label_column] == 0].sample(n_samples_per_label)\n",
        "# df = pd.concat([label_1_rows, label_0_rows])\n",
        "\n",
        "# # Filter out irrelevant characters\n",
        "# df = df[df[text_column].apply(lambda x: has_meaningful_text(x))]\n",
        "\n",
        "# # Example usage\n",
        "# for i, row in df.iterrows():\n",
        "#     if has_meaningful_text(row[text_column]):\n",
        "#         print(f\"Row {i} contains meaningful text.\")\n",
        "#     else:\n",
        "#         print(f\"Row {i} does not contain meaningful text.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pandas as pd\n",
        "# import spacy\n",
        "# import random\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# # Prompt user for the file location\n",
        "# file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# # Load the .csv file as a DataFrame\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# # Prompt user for the column names\n",
        "# label_column = input(\"Enter the column name for labels: \")\n",
        "# text_column = input(\"Enter the column name for text: \")\n",
        "\n",
        "# def has_meaningful_text(email_text):\n",
        "#     doc = nlp(email_text)\n",
        "#     noun_count = sum([1 for token in doc if token.pos_ == \"NOUN\"])\n",
        "#     return noun_count >= 10  # arbitrary threshold for number of nouns\n",
        "\n",
        "# # Filter out irrelevant characters\n",
        "# df_meaningful = df[df[text_column].apply(lambda x: has_meaningful_text(x))]\n",
        "\n",
        "# # Downsample the dataset to select 6000 rows at random\n",
        "# n_samples = 6000\n",
        "# label_counts = df_meaningful[label_column].value_counts()\n",
        "# n_samples_per_label = int(n_samples / 2)\n",
        "# label_1_rows = df_meaningful[df_meaningful[label_column] == 1].sample(n_samples_per_label)\n",
        "# label_0_rows = df_meaningful[df_meaningful[label_column] == 0].sample(n_samples_per_label)\n",
        "# df_sampled = pd.concat([label_1_rows, label_0_rows])\n",
        "\n",
        "# # Save the downsampled DataFrame as a new CSV file\n",
        "# output_file_path = os.path.splitext(file_path)[0] + \"_Dwn_Smpl_6000.csv\"\n",
        "# df_sampled.to_csv(output_file_path, index=False)\n",
        "\n",
        "# # Example usage\n",
        "# print(f\"The original DataFrame contains {len(df)} rows.\")\n",
        "# print(f\"The df_meaningful DataFrame contains {len(df_meaningful)} rows.\")\n",
        "# print(f\"The df_sampled DataFrame contains {len(df_sampled)} rows.\")\n",
        "# print(f\"The downsampled DataFrame has been saved to {output_file_path}.\")\n"
      ],
      "metadata": {
        "id": "OGEPlVXfopV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import random\n",
        "\n",
        "# # Prompt user for the file location\n",
        "# file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# # Load the .csv file as a DataFrame\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# # Prompt user for the column name\n",
        "# text_column = input(\"Enter the column name: \")\n",
        "\n",
        "# # Randomly sample 25 rows from the DataFrame\n",
        "# df_sample = df.sample(n=25, random_state=42)\n",
        "\n",
        "# # Count the number of words in each row of the specified column\n",
        "# df_sample[text_column+\"_wrd_count\"] = df_sample[text_column].apply(lambda x: len(x.split()))\n",
        "\n",
        "# # Example usage\n",
        "# print(f\"The original DataFrame contains {len(df)} rows.\")\n",
        "# print(f\"The new word count column is named '{text_column}_wrd_count'.\")\n",
        "# print(df_sample.head())\n"
      ],
      "metadata": {
        "id": "-NHjfdf7BV29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script loads a .csv file as a pandas DataFrame, prompts the user for a text column name, \n",
        "randomly samples 25 rows from the DataFrame, and counts the number of words in each row of the specified column.\n",
        "The resulting DataFrame with the word count column added is then printed to the console.\n",
        "\n",
        "Author: Robert Houston\n",
        "Date: 3/19/2023\n",
        "\n",
        "Usage: python sample_and_count.py\n",
        "\n",
        "Requirements: pandas\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Prompt user for the file location\n",
        "file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# Load the .csv file as a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prompt user for the column name\n",
        "text_column = input(\"Enter the column name: \")\n",
        "\n",
        "# Randomly sample 25 rows from the DataFrame\n",
        "df_sample = df.sample(n=25, random_state=42)\n",
        "\n",
        "# Count the number of words in each row of the specified column\n",
        "df_sample[text_column+\"_wrd_count\"] = df_sample[text_column].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Print summary of sampled and counted data\n",
        "print(f\"The original DataFrame contains {len(df)} rows.\")\n",
        "print(f\"The new word count column is named '{text_column}_wrd_count'.\")\n",
        "print(df_sample.head())"
      ],
      "metadata": {
        "id": "kDiECJyvX-xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "id": "kY-4iRIGDNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "id": "a4_yHxdVGGoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Working Language Detection Code\n",
        "# import pandas as pd\n",
        "# from langdetect import detect_langs\n",
        "\n",
        "# # Prompt user for the file location\n",
        "# file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# # Load the .csv file as a DataFrame\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# # Prompt user for the column name\n",
        "# text_column = input(\"Enter the column name: \")\n",
        "\n",
        "# # Define function to check if text is in English\n",
        "# def is_english(text):\n",
        "#     try:\n",
        "#         languages = detect_langs(text)\n",
        "#         for lang in languages:\n",
        "#             if lang.lang == \"en\" and lang.prob > 0.90:\n",
        "#                 return True\n",
        "#         return False\n",
        "#     except:\n",
        "#         return False\n",
        "\n",
        "# # Drop rows that are not in English\n",
        "# df = df[df[text_column].apply(lambda x: is_english(x))]\n",
        "\n",
        "# # Example usage\n",
        "# print(f\"The modified DataFrame contains {len(df)} rows.\")\n",
        "# print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Tr3bj2GWgG",
        "outputId": "1f5753dd-ea1d-4ac3-9d53-bb4123bc99ef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file location: /content/drive/MyDrive/Production Datasets/GOLD_DeDup_Squeeky_Clean.csv\n",
            "Enter the column name: clean_stage_1\n",
            "The modified DataFrame contains 86 rows.\n",
            "                                   subject  \\\n",
            "0                      good question ( s )   \n",
            "2                                 re [ 8 ]   \n",
            "3  hey , it ' s me beckyvgko 968124 on msn   \n",
            "4  etc - theatre event - cirque due soleil   \n",
            "5       lst rev dec . 1999 josey ranch nom   \n",
            "\n",
            "                                                text  label  \\\n",
            "0                         i can answer some of these      0   \n",
            "2  yes , i don ' t mind outlaw star in 1990 ask o...      1   \n",
            "3  jkoutsi if the image above does not open pleas...      1   \n",
            "4  please note that i need headcount extra early ...      0   \n",
            "5  fyi forwarded by susan d trevino / hou / ect o...      0   \n",
            "\n",
            "                                       clean_stage_1  \\\n",
            "0                         i can answer some of these   \n",
            "2  yes , i don ' t mind outlaw star in 1990 ask o...   \n",
            "3  jkoutsi if the image above does not open pleas...   \n",
            "4  please note that i need headcount extra early ...   \n",
            "5  fyi forwarded by susan d trevino / hou / ect o...   \n",
            "\n",
            "                                          clean_text  JJ  NN  VB  \n",
            "0                                             answer   0   1   0  \n",
            "2     yes mind outlaw star 1990 ask therepoetry 1994   0   5   0  \n",
            "3  jkoutsi image open please go site important in...   4  12   2  \n",
            "4  please note need headcount extra early event !...  30  65  16  \n",
            "5  fyi forwarded susan trevino hou ect 12 15 99 0...  13  58  13  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "65e58kVyDi4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script loads a .csv file as a pandas DataFrame, prompts the user for a text column name, and then drops all non-English rows in that column using the langdetect library. The cleaned DataFrame is saved in a new file with \"_Eng_Only\" appended to the original file name in the same directory as the source file.\n",
        "\n",
        "Author: Robert Houston\n",
        "Date: 3/19/2023\n",
        "Usage: python clean_text_data.py\n",
        "\n",
        "Requirements: pandas, langdetect\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from langdetect import detect_langs\n",
        "import os\n",
        "\n",
        "# Prompt user for the file location\n",
        "file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# Load the .csv file as a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prompt user for the column name\n",
        "text_column = input(\"Enter the column name: \")\n",
        "\n",
        "# Define function to check if text is in English\n",
        "def is_english(text):\n",
        "    \"\"\"\n",
        "    This function takes a text string as input and returns True if it is in English, False otherwise.\n",
        "    It uses the langdetect library to detect the language of the text and checks if the detected language\n",
        "    is English with a probability score of at least 0.90.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        languages = detect_langs(text)\n",
        "        for lang in languages:\n",
        "            if lang.lang == \"en\" and lang.prob > 0.90:\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Drop rows that are not in English\n",
        "df = df[df[text_column].apply(lambda x: is_english(x))]\n",
        "\n",
        "# Save cleaned DataFrame to a new file\n",
        "filename, ext = os.path.splitext(file_path)\n",
        "new_filename = f\"{filename}_Eng_Only{ext}\"\n",
        "df.to_csv(new_filename, index=False)\n",
        "\n",
        "# Print summary of cleaned data\n",
        "print(f\"The modified DataFrame contains {len(df)} rows.\")\n",
        "print(df.head())\n",
        "\n",
        "# Print location of cleaned data file\n",
        "print(f\"The cleaned data has been saved in {new_filename}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuTEhXsJWV7E",
        "outputId": "06d439e2-da2c-454d-e5ee-a19f5e118608"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file location: /content/drive/MyDrive/Production Datasets/GOLD_DeDup.csv\n",
            "Enter the column name: text\n",
            "The modified DataFrame contains 39240 rows.\n",
            "                                    subject  \\\n",
            "0                   + cialis online - = + _   \n",
            "3  executive solicitation - united way 2000   \n",
            "4            inncenot bitcehs gushing loads   \n",
            "5           fw : stupid girly and huge ccok   \n",
            "7    re : shy amateur girls toy ass 2 mouth   \n",
            "\n",
            "                                                text  label  \n",
            "0  ' ' ' ' ' ' ' ' ' ' buy meds from us and save ...      1  \n",
            "3  \u0001 & who wants to help millions ? \u0001 8 is our th...      0  \n",
            "4  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "5  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "7  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "The cleaned data has been saved in /content/drive/MyDrive/Production Datasets/GOLD_DeDup_Eng_Only.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fuzzywuzzy python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7B2jTkgsj_t",
        "outputId": "cc353435-da44-4215-9733-4a24487e4b88"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.9\n",
            "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.9 fuzzywuzzy-0.18.0 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langdetect import detect_langs\n",
        "import os\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Prompt user for the file location\n",
        "file_path = input(\"Enter the file location: \")\n",
        "\n",
        "# Load the .csv file as a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prompt user for the column name\n",
        "text_column = input(\"Enter the column name: \")\n",
        "\n",
        "# # Define function to check if text is in English\n",
        "# def is_english(text):\n",
        "#     \"\"\"\n",
        "#     This function takes a text string as input and returns True if it is in English, False otherwise.\n",
        "#     It uses the langdetect library to detect the language of the text and checks if the detected language\n",
        "#     is English with a probability score of at least 0.90.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         languages = detect_langs(text)\n",
        "#         for lang in languages:\n",
        "#             if lang.lang == \"en\" and lang.prob > 0.90:\n",
        "#                 return True\n",
        "#         return False\n",
        "#     except:\n",
        "#         return False\n",
        "\n",
        "# # Drop rows that are not in English\n",
        "# df = df[df[text_column].apply(lambda x: is_english(x))]\n",
        "\n",
        "# # Define function to remove highly similar rows\n",
        "# def remove_similar_rows(df, column, threshold):\n",
        "#     indices_to_remove = set()\n",
        "\n",
        "#     for i in range(len(df)):\n",
        "#         if i not in indices_to_remove:\n",
        "#             for j in range(i+1, len(df)):\n",
        "#                 if j not in indices_to_remove:\n",
        "#                     similarity = fuzz.token_sort_ratio(df.iloc[i][column], df.iloc[j][column])\n",
        "#                     if similarity >= threshold:\n",
        "#                         indices_to_remove.add(j)\n",
        "\n",
        "#     return df.drop(list(indices_to_remove))\n",
        "\n",
        "# # Remove highly similar rows\n",
        "# similarity_threshold = 90\n",
        "# df = remove_similar_rows(df, text_column, similarity_threshold)\n",
        "\n",
        "# Drop duplicate rows in the specified column\n",
        "df = df.drop_duplicates(subset=[text_column])\n",
        "\n",
        "# Save cleaned DataFrame to a new file\n",
        "filename, ext = os.path.splitext(file_path)\n",
        "new_filename = f\"{filename}_Eng_Only{ext}\"\n",
        "df.to_csv(new_filename, index=False)\n",
        "\n",
        "# Print summary of cleaned data\n",
        "print(f\"The modified DataFrame contains {len(df)} rows.\")\n",
        "print(df.head())\n",
        "\n",
        "# Print location of cleaned data file\n",
        "print(f\"The cleaned data has been saved in {new_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U01eRxigsi-7",
        "outputId": "56ca5df8-5a04-40f4-ff11-c36fd741ce55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file location: /content/drive/MyDrive/Production Datasets/Old_GOLD_DeDup_Eng_Only_Eng_Only.csv\n",
            "Enter the column name: subject\n",
            "The modified DataFrame contains 26551 rows.\n",
            "                                    subject  \\\n",
            "0                   + cialis online - = + _   \n",
            "1  executive solicitation - united way 2000   \n",
            "2            inncenot bitcehs gushing loads   \n",
            "3           fw : stupid girly and huge ccok   \n",
            "4    re : shy amateur girls toy ass 2 mouth   \n",
            "\n",
            "                                                text  label  \n",
            "0  ' ' ' ' ' ' ' ' ' ' buy meds from us and save ...      1  \n",
            "1  \u0001 & who wants to help millions ? \u0001 8 is our th...      0  \n",
            "2  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "3  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "4  ' ello , ' ello , ' ello , what have we got he...      1  \n",
            "The cleaned data has been saved in /content/drive/MyDrive/Production Datasets/Old_GOLD_DeDup_Eng_Only_Eng_Only_Eng_Only.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines two new functions: generate_similar_text and generate_synthetic_data. The generate_similar_text function takes a text string and a similarity threshold as input, and generates a new text string with a similarity equal to or greater than the given threshold. The generate_synthetic_data function takes a DataFrame, a column name, the number of synthetic samples to generate, and a similarity threshold as input, and generates a new DataFrame containing synthetic data.\n",
        "\n",
        "The code then generates synthetic data, concatenates it with the original data, and saves the resulting balanced DataFrame to a new file. You can adjust the num_synthetic_samples and similarity_threshold variables to control the number of synthetic samples and the desired similarity, respectively."
      ],
      "metadata": {
        "id": "duCihxUJ4JFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import textdistance\n",
        "\n",
        "def generate_similar_text(text, similarity_threshold):\n",
        "    generated_text = \"\".join([c if random.random() < similarity_threshold else random.choice(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \") for c in text])\n",
        "    similarity = textdistance.jaccard(text, generated_text)\n",
        "    \n",
        "    while similarity < similarity_threshold:\n",
        "        generated_text = \"\".join([c if random.random() < similarity_threshold else random.choice(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \") for c in text])\n",
        "        similarity = textdistance.jaccard(text, generated_text)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def generate_synthetic_data(df, column, num_samples, similarity_threshold):\n",
        "    synthetic_data = []\n",
        "    for _ in range(num_samples):\n",
        "        original_text = random.choice(df[column].tolist())\n",
        "        generated_text = generate_similar_text(original_text, similarity_threshold)\n",
        "        synthetic_data.append(generated_text)\n",
        "    return pd.DataFrame(synthetic_data, columns=[column])\n",
        "\n",
        "# Generate synthetic data\n",
        "num_synthetic_samples = 100\n",
        "similarity_threshold = 0.75\n",
        "synthetic_df = generate_synthetic_data(df, text_column, num_synthetic_samples, similarity_threshold)\n",
        "\n",
        "# Concatenate the original and synthetic data\n",
        "balanced_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
        "\n",
        "# Save the balanced DataFrame to a new file\n",
        "filename, ext = os.path.splitext(file_path)\n",
        "new_filename = f\"{filename}_Balanced{ext}\"\n",
        "balanced_df.to_csv(new_filename, index=False)\n",
        "\n",
        "# Print summary of balanced data\n",
        "print(f\"The balanced DataFrame contains {len(balanced_df)} rows.\")\n",
        "print(balanced_df.head())\n",
        "\n",
        "# Print location of balanced data file\n",
        "print(f\"The balanced data has been saved in {new_filename}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YoRTY3-kxWXU",
        "outputId": "25178b57-3344-47d6-9763-04ca7eb3ef34"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3a8b0e146044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_similar_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msimilarity_threshold\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textdistance'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}